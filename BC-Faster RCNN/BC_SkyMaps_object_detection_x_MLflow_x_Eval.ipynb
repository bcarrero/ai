{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MH_SkyMaps_object_detection_x_MLflow_x_Eval.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSWkSKG3e6dz"
      },
      "source": [
        "# Introductory notebook for Omdena Skymaps\n",
        "\n",
        "Hi everybody! As I saw most of the participants want to use the colab, I prepared this notebook - it is the code from scripts predict_png.py and train.py contained in the baseline Github repository, as these are the two important ones. Please, work your way through it, try to understand the code and play around with the data. We plan to share some specific tasks soon, but first we need everybody to get going. This may result in slow start for advanced participants, but please be patient - its also a first Omdena challenge for us at Skymaps and we are trying our best so everyone has the chance to learn something and also enjoy it!\n",
        "\n",
        "We are assuming everyone has at least basic knowledge of Python, jupyter notebooks / google colab and machine learning. If you do not and feel completely lost, please try to do some short online python crash course and study how to execute python code. I believe you can still learn something, but it may require a lot of determination on your side.\n",
        "\n",
        "## Instruction for debugging\n",
        "\n",
        "\n",
        "I saw many people trying to debug some code / asking for help. If you are stuck and ask for help, make sure your question contains the following:\n",
        "\n",
        "*   Before you ask for help, search the Slack whether similar problem has not been already solved\n",
        "*   Before you ask for help, search the error message on Google \n",
        "*   Clearly state the problem\n",
        "*   Tell us what you already did to solve the issue\n",
        "*   Post a screenshot of your error message\n",
        "*   Post execution environment (colab, my PC ...)\n",
        "\n",
        "This not only helps others to solve your problem, but also keeps slack from getting spammed and most importantly - you learn most by searching for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibwxEftxjgKa"
      },
      "source": [
        "# CODE STARTS HERE\n",
        "\n",
        "## Before you start working, switch your instance to GPU acceleration!\n",
        "\n",
        "This can be done by Edit > Notebook settings or Runtime>Change runtime type and select GPU as Hardware accelerator.\n",
        "\n",
        "Firstly we need to setup the environment (output ommited by using %%capture) - if you have trouble  running cells with %%capture, delete this row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtNdJxXZmMbE"
      },
      "source": [
        "# To avoid lots of log prints\n",
        "%%capture  \n",
        "\n",
        "!pip install numpy scipy Pillow matplotlib scikit-image scikit-learn \n",
        "!pip install opencv-python torch geopandas geojson \n",
        "!pip install rasterio rio_tiler torchvision supermercado mercantile gdown\n",
        "!pip install albumentations==0.5.2\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "!pip install mlflow --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e2oFz1XCtNV"
      },
      "source": [
        "\n",
        "We are installing COCO Python API for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZzhVMBCqpB"
      },
      "source": [
        "%%capture\n",
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "# Install pycocotools, the version by default in Colab\n",
        "# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rknePfHvr2ZD"
      },
      "source": [
        "Next lets create the project folder structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJJAoIsKh7DA"
      },
      "source": [
        "**Use own Google Drive to avoid reinitializing everything everytime**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuApktn5hvTp"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ooHsbGrr77O"
      },
      "source": [
        "!rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KoVChlrfN3I"
      },
      "source": [
        "!mkdir SkyMaps_task4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4URWFbESXsc"
      },
      "source": [
        "%cd SkyMaps_task4/\n",
        "!mkdir data\n",
        "!mkdir weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYrR1i2hs4UD"
      },
      "source": [
        "Download dataset containing training and validation image tiles including annotations (output ommited by using %%capture) and also the trained baseline Faster-RCNN model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wz3rzantl_b"
      },
      "source": [
        "# To avoid lots of log prints\n",
        "%%capture  \n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1nGVhW9mgvonHBsaDn6Q0-yCK8CCCvKX1 -O ./data/complete_dataset_rgb.zip\n",
        "# !gdown https://drive.google.com/uc?id=1HheG86uj1_RsgfwJ1X1WIsT9Iwh430jY -O ./weights/fasterrcnn_repa.pth\n",
        "!unzip ./data/complete_dataset_rgb.zip -d ./data\n",
        "\n",
        "!rm -rf ./data/complete_dataset_rgb.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BdYtsUqhC6e"
      },
      "source": [
        "**Clone the code base**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut8adfUmgGAv"
      },
      "source": [
        "import mlflow\n",
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "from getpass import getpass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiAunxYSDKae"
      },
      "source": [
        "access_token = getpass('Enter your GitHub access token: ')\n",
        "\n",
        "! git clone https://{access_token}@github.com/OmdenaAI/SkyMaps.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywn0CQ59VdFK"
      },
      "source": [
        "%cd SkyMaps\n",
        "! git pull\n",
        "%ls\n",
        "! git branch -a\n",
        "! git branch t4-od-bc-001\n",
        "! git checkout t4-od-bc-001\n",
        "! git branch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po4oVucXfokH"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUrH4z2HBYZY"
      },
      "source": [
        "## Configure MLflow ðŸ§"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eHQv2jBwD5"
      },
      "source": [
        "**Set Environment Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBELakGtBxA1"
      },
      "source": [
        "#@title Enter the repository name for the project:\n",
        "\n",
        "REPO_NAME= \"ai/test\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuANw2qOB0Bn"
      },
      "source": [
        "#@title Enter the username of your DAGsHub account:\n",
        "\n",
        "USER_NAME = \"bcarrero\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nxrxYWbB6j3"
      },
      "source": [
        "**Initialize MLflow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRj24etBCIFY"
      },
      "source": [
        "**Set Local Configurations**\n",
        "\n",
        "Under the [Token tab](https://dagshub.com/user/settings/tokens) in the user setting, copy the default token and use it here.\n",
        "\n",
        "Could make it work if 2FA is enabled on DagsHub. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1iscLEWQPiL"
      },
      "source": [
        "# When it says \"copy the default token and used it here\"... Where exactly is here? Is this needed?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZBqtl1CKAy"
      },
      "source": [
        "os.environ['MLFLOW_TRACKING_USERNAME'] = USER_NAME\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = getpass('Enter your DAGsHub access token or password: ')\n",
        "# token = getpass('Enter your DAGsHub access only token: ')\n",
        "\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/{USER_NAME}/{REPO_NAME}.mlflow')\n",
        "# mlflow.set_tracking_uri(f'https://{token}@dagshub.com/{USER_NAME}/{REPO_NAME}.mlflow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYf3-E6wyEWa"
      },
      "source": [
        "## Predict crop on a single PNG image\n",
        "\n",
        "Following code is to detect crop on a single PNG image tile using a trained baseline Faster-RCNN model. Firstly import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4wvfqqbyf69"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyLy2OBK4lhU"
      },
      "source": [
        "Now lets define some paths and constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vROZDVNJ4noh"
      },
      "source": [
        "DIR_INPUT = '/content/SkyMaps_task4/data/complete_dataset'\n",
        "DIR_TRAIN = f'{DIR_INPUT}/train'\n",
        "DIR_VAL = f'{DIR_INPUT}/validation'\n",
        "DIR_TEST = f'{DIR_INPUT}/test'\n",
        "my_model_filepath = '../SkyMaps_task4/weights/fasterrcnn_resnet152_fpn.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4A8GJ-W3Rhx"
      },
      "source": [
        "# Train your own model\n",
        "\n",
        "### Now lets train your own model using the same procedure as the baseline model was trained! Lets import some more libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_N-ut-z3pRj"
      },
      "source": [
        "import albumentations as A\n",
        "import pandas as pd\n",
        "import IPython\n",
        "import torch\n",
        "import torchvision\n",
        "import pytorch_lightning\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EopBLTG2Yj9Q"
      },
      "source": [
        "import sys\n",
        "sys.path.append('ml-detection/models')\n",
        "\n",
        "from l_faster_rcnn import fasterrcnn_resnet152_fpn, fasterrcnn_resnext50_32x4d_fpn, evaluate\n",
        "from l_Evaluation import COCOEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfelLtFZ45wz"
      },
      "source": [
        "### Before we load the training annotations, lets declare a function that we will use to process them to a correct format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRJZoI6q5CE0"
      },
      "source": [
        "def process_annotations(input_df, print_shape=False, ordinal_encoder=None):\n",
        "    \"\"\" Function to process input annotations from Tensorflow object detection format before training\n",
        "    \"\"\"\n",
        "\n",
        "    # Define pandas columns for bounding box position\n",
        "    input_df = input_df.rename(columns={'filename': 'image_id', 'xmin': 'x', 'ymin': 'y'})\n",
        "    input_df['w'] = input_df['xmax'] - input_df['x']\n",
        "    input_df['h'] = input_df['ymax'] - input_df['y']\n",
        "    input_df = input_df.drop(columns=['xmax', 'ymax'])\n",
        "\n",
        "    input_df['x'] = input_df['x'].astype(np.float)\n",
        "    input_df['y'] = input_df['y'].astype(np.float)\n",
        "    input_df['w'] = input_df['w'].astype(np.float)\n",
        "    input_df['h'] = input_df['h'].astype(np.float)\n",
        "\n",
        "    # Encode classes using ordinal encoding +1\n",
        "    if ordinal_encoder is None:\n",
        "      ordinal_encoder = preprocessing.LabelEncoder()\n",
        "      ordinal_encoder.fit(input_df['class'].unique())\n",
        "    input_df['class'] = ordinal_encoder.transform(input_df['class']) + 1\n",
        "\n",
        "    image_ids = input_df['image_id'].unique()\n",
        "\n",
        "    if print_shape:\n",
        "        print(f'Loaded dataframe of shape {input_df.shape}')\n",
        "\n",
        "    return input_df, image_ids, ordinal_encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D3g5tQk56sa"
      },
      "source": [
        "Now we can define augmentations function from the Albumentations library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vtMjAYd6BMk"
      },
      "source": [
        "# Albumentations augmentations functions\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.Flip(0.25),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.Transpose(p=0.25),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.RandomGamma(p=0.2),\n",
        "        A.Blur(p=0.2),\n",
        "        A.ColorJitter(p=0.2),\n",
        "        A.Downscale(p=0.2),\n",
        "        A.ChannelDropout(p=0.2),\n",
        "        A.ChannelShuffle(p=0.2),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvehRL91FNaT"
      },
      "source": [
        "# Albumentation validation transform - simple conversion to pytorch Tensor\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUmuNBfc6Hy6"
      },
      "source": [
        "Now lets declare three classes - first is a Pytorch dataset class for data loading during model training, the other two are classes that help us during the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcCo_w3E6bW4"
      },
      "source": [
        "class PlantDataset(Dataset):\n",
        "    \"\"\" Pytorch dataset class customized for Skymaps data loading\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = dataframe['image_id'].unique()\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['image_id'] == image_id]\n",
        "\n",
        "        image = cv2.imread(f'{self.image_dir}/{image_id}', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "\n",
        "        # Create torch of ones and assign class by multipy it with class label\n",
        "        labels = torch.ones((records.shape[0],), dtype=torch.int64) * torch.tensor(records['class'].values)\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        # target['masks'] = None\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "\n",
        "        height, width, channels = image.shape\n",
        "        target['width'] = torch.tensor([width])\n",
        "        target['height'] = torch.tensor([height])\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "\n",
        "            # target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "            target['boxes'] = torch.tensor(sample['bboxes'])\n",
        "\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY2-n_F8FSgk"
      },
      "source": [
        "class Averager:\n",
        "    \"\"\" Class for averaging loss function during training process\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwPLtgf_FUKd"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\" Batch data loading helper function for pytorch\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3gztMGk5D_t"
      },
      "source": [
        "Now we can load the annotations and process them to correct input format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrOCeuy65IKB"
      },
      "source": [
        "train_df = pd.read_csv(f'{DIR_TRAIN}/_annotations.csv')\n",
        "valid_df = pd.read_csv(f'{DIR_VAL}/_annotations.csv')\n",
        "test_df = pd.read_csv(f'{DIR_TEST}/_annotations.csv')\n",
        "\n",
        "print('Loaded annotations before format conversion')\n",
        "print(train_df.head(5).to_string())\n",
        "\n",
        "train_df, train_img_ids, ordinal_encoder = process_annotations(train_df, print_shape=True)\n",
        "valid_df, val_img_ids, _ = process_annotations(valid_df, print_shape=True, ordinal_encoder=ordinal_encoder)\n",
        "test_df, test_img_ids, _ = process_annotations(test_df, print_shape=True, ordinal_encoder=ordinal_encoder)\n",
        "\n",
        "print('\\nAnnotations formatted')\n",
        "print(train_df.head(5).to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYpIKCAm2gTA"
      },
      "source": [
        "Initialize a model and provide some basic settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB65jJzAjZtE"
      },
      "source": [
        "# Create dictionaries for the detected crops (this is used only if plotting is needed)\n",
        "class_dict = {k:v for k, v in enumerate([\"background\"] + list(ordinal_encoder.classes_))}\n",
        "\n",
        "# Set number of classes\n",
        "num_classes = len(class_dict) # background + classes\n",
        "\n",
        "# Set prediction threshold - only instances with higher probability will be assigned as detected\n",
        "prediction_threshold = 0.3\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PQPzyVuCUYw"
      },
      "source": [
        "Create an instance of a new model with correct settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWZUAxjZ6iQm"
      },
      "source": [
        "# load a model; pre-trained on COCO\n",
        "\n",
        "my_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, box_score_thresh=prediction_threshold)\n",
        "# my_model = fasterrcnn_resnet152_fpn(pretrained=False, box_score_thresh=prediction_threshold)\n",
        "# my_model = fasterrcnn_resnext50_32x4d_fpn(pretrained=False, box_score_thresh=prediction_threshold)\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = my_model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "my_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrJIvIntAys_"
      },
      "source": [
        "Now lets  check the execution device \n",
        "- GPU / CPU\n",
        "and \n",
        "- setup training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK8RGsIG6sqf"
      },
      "source": [
        "# Get device - cuda / CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Device {device} is used for training')\n",
        "my_model.to(device)\n",
        "params = [p for p in my_model.parameters() if p.requires_grad]\n",
        "# Set training hyperparameters\n",
        "num_epochs = 20\n",
        "num_eval_step = 10\n",
        "\n",
        "learning_rate = 0.0005\n",
        "early_stop_limit = 20\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay=0.0005)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "lr_scheduler = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AxHuj-jBT9J"
      },
      "source": [
        "Define training and validation datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIuifHEnBTU3"
      },
      "source": [
        "# Create train and validation train datasets\n",
        "# def __init__(self, dataframe, image_dir, transforms=None):\n",
        "train_dataset = PlantDataset(train_df, DIR_TRAIN, get_train_transform())  \n",
        "valid_dataset = PlantDataset(valid_df, DIR_VAL, get_valid_transform())\n",
        "test_dataset = PlantDataset(test_df, DIR_TEST, get_valid_transform())\n",
        "\n",
        "batch_size = 4\n",
        "# Create train dataloader with settings\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Create test dataloader with settings\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Create validation dataloader with settings\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYIUABvDCb5s"
      },
      "source": [
        "## Set MLflow Auto-Logging\n",
        "\n",
        "[mlflow.pytorch.autolog](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#mlflow.pytorch.autolog) only work with PyTorch Lightning only. So, it has no impact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AHjLNhICc5I"
      },
      "source": [
        " mlflow.pytorch.autolog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYhPrU5TCmoP"
      },
      "source": [
        "## See the Experiment Results - LIVE! ðŸ“³\n",
        "\n",
        "In this tab, you can see the results of the experiment while it's running!\n",
        "\n",
        "**Notice**: To update the experiment status, simply go back to the \"Experiment Tab\" and reopen the top experiment in the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no1KY_deCpzD"
      },
      "source": [
        "print(f\"https://dagshub.com/{USER_NAME}/{REPO_NAME}/experiments/#/\")\n",
        "display(IPython.display.IFrame(f\"https://dagshub.com/{USER_NAME}/{REPO_NAME}/experiments/#/\",'100%',600))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcEqNBUDBYyx"
      },
      "source": [
        "## **Training**\n",
        "And now lets start the **training loop**! The training will take some time even on GPU - usually 10-30 minutes depending on the settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INUVQQw96wIn"
      },
      "source": [
        "# Create averager classes - only to keep track of loss function value during training\n",
        "train_loss_hist = Averager()\n",
        "val_loss_hist = Averager()\n",
        "\n",
        "print('-------------- Start training -----------------')\n",
        "\n",
        "# Training loop initializers\n",
        "early_stop_counter = 0\n",
        "best_valid_loss = 100.0\n",
        "\n",
        "evaluator = None\n",
        "\n",
        "# Train the model\n",
        "with mlflow.start_run() as run:\n",
        "  mlflow.log_params({\"epochs\": num_epochs, \"learning_rate\": learning_rate, \n",
        "                     \"batch_size\": batch_size})\n",
        "  for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "\n",
        "      train_loss_hist.reset()\n",
        "      val_loss_hist.reset()\n",
        "\n",
        "      # Loop through the training dataset\n",
        "      for images, targets, train_img_ids in train_data_loader:\n",
        "          # Load input images\n",
        "          images = list(image.to(device) for image in images)\n",
        "          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "          # Create loss dictionary\n",
        "          loss_dict = my_model(images, targets)\n",
        "          # Calculate sum of losses over one epoch\n",
        "          losses = sum(loss for loss in loss_dict.values())\n",
        "          loss_value = losses.item()\n",
        "          train_loss_hist.send(loss_value)\n",
        "          # Backpropagate the errors and update the weights\n",
        "          optimizer.zero_grad()\n",
        "          losses.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # Calculate the model loss on validation data\n",
        "      if (epoch + 1) % num_eval_step == 0:\n",
        "        _, ground_truth_dict, predictions_dict = evaluate(my_model, valid_data_loader, device, val_loss_hist, mode='Comb')\n",
        "\n",
        "        # calling COCO Evaluation on Test Dataset\n",
        "        if evaluator is None:\n",
        "          evaluator = COCOEvaluator(ground_truth=ground_truth_dict, categories=class_dict, data_type='robo')\n",
        "        \n",
        "        ml_stats = {\"loss\": train_loss_hist.value, \"val_loss\": val_loss_hist.value}\n",
        "        stats = evaluator.Evaluate(detections=predictions_dict)\n",
        "        ml_stats.update(stats)\n",
        "      else:\n",
        "        evaluate(my_model, valid_data_loader, device, val_loss_hist)\n",
        "        ml_stats = {\"loss\": train_loss_hist.value, \"val_loss\": val_loss_hist.value}\n",
        "\n",
        "      # update the learning rate - could be used in the future (now set to none)\n",
        "      if lr_scheduler is not None:\n",
        "          lr_scheduler.step()\n",
        "\n",
        "      print(f\"Epoch #{epoch} train loss: {train_loss_hist.value} valid loss: {val_loss_hist.value} time: {time.time() - start_time}\")\n",
        "\n",
        "      # ML flow logging is done here\n",
        "      mlflow.log_metrics(ml_stats, step=epoch)\n",
        "\n",
        "      # Check if validation loss is less than the previous best val loss and, if so, save best model\n",
        "      if best_valid_loss > val_loss_hist.value:\n",
        "          best_valid_loss = val_loss_hist.value\n",
        "          print('Best achieved validation loss, saving model')\n",
        "          torch.save(my_model.state_dict(), my_model_filepath)\n",
        "          early_stop_counter = 0\n",
        "      else:\n",
        "          early_stop_counter += 1\n",
        "      # If the validation loss has not increased for the specified number of epochs, stop the training loop\n",
        "      if early_stop_counter > early_stop_limit:\n",
        "          print(f'Validation loss has not increased for {early_stop_limit} epochs, stopping training loop')\n",
        "          break\n",
        "\n",
        "print('-------------- Training finished  -----------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyCcRndQJIlQ"
      },
      "source": [
        "## Print MLflow data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_o0_x7SVCrM"
      },
      "source": [
        "from mlflow.tracking import MlflowClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg074ni1JHg4"
      },
      "source": [
        "def print_auto_logged_info(r):\n",
        "\n",
        "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
        "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
        "    print(\"run_id: {}\".format(r.info.run_id))\n",
        "    print(\"artifacts: {}\".format(artifacts))\n",
        "    print(\"params: {}\".format(r.data.params))\n",
        "    print(\"metrics: {}\".format(r.data.metrics))\n",
        "    print(\"tags: {}\".format(tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flt0F5KaI529"
      },
      "source": [
        "# fetch the auto logged parameters and metrics\n",
        "print_auto_logged_info(mlflow.get_run(run_id=run.info.run_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_N2Pvp_jCL5"
      },
      "source": [
        "## Start Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq3QCKNgrCWy"
      },
      "source": [
        "# evaluate on the test dataset\n",
        "_, ground_truth_dict, predictions_dict = evaluate(my_model, test_data_loader, device=device, mode='Test')\n",
        "\n",
        "# calling COCO Evaluation on Test Dataset\n",
        "evaluator = COCOEvaluator(ground_truth=ground_truth_dict, categories=class_dict, data_type='robo')\n",
        "evaluator.Evaluate(detections=predictions_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPnCZAc7Z67R"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWJlQ1bpoGG3"
      },
      "source": [
        "Load a sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIx05PrGnT1_"
      },
      "source": [
        "# Choose a png file for prediction and visualisation\n",
        "# You can choose any image file in the data/beetroot/train/validation\n",
        "image_file = 'repa_trn_23_4598964_2920955_png_jpg.rf.e4c6865c435d89fc09a7307f168a5929.jpg'\n",
        "pred_file = os.path.join(DIR_VAL, image_file)\n",
        "\n",
        "# Read the image using cv2\n",
        "image = cv2.imread(pred_file, cv2.IMREAD_COLOR)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z5OQFWrnzRw"
      },
      "source": [
        "convert image to Torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMk_rqJ9n1KY"
      },
      "source": [
        "# Convert the image to torch tensor and send it to the processing device\n",
        "image_torch = np.expand_dims(np.transpose(image, (2, 0, 1)), axis=0)\n",
        "image_torch /= 255.0\n",
        "image_torch = torch.tensor(image_torch).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beB1Wpfa200p"
      },
      "source": [
        "Now lets declare function to pretty print the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWdHK1q30Nzk"
      },
      "source": [
        "def draw_text(img, text, font=cv2.FONT_HERSHEY_PLAIN, pos=(0, 0), font_scale=0.6, font_thickness=1,\n",
        "              text_color=(0, 0, 0), text_color_bg=(0, 0, 0)):\n",
        "    \"\"\" Function to pretty print label names above the bounding box\n",
        "    \"\"\"\n",
        "    x, y = pos\n",
        "    text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
        "    text_w, text_h = text_size\n",
        "    cv2.rectangle(img, pos, (x + text_w, y + text_h), text_color_bg, -1)\n",
        "    cv2.putText(img, text, (x, y + text_h + font_scale - 1), font, font_scale, text_color, font_thickness)\n",
        "\n",
        "    return text_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqYQw_wABfcJ"
      },
      "source": [
        "**By now you should get a trained model - congratulations! We can visualise its predictions on a single validation PNG tile**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjyqHFQCoMpn"
      },
      "source": [
        "my_model = my_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bqhuS7WFFr1"
      },
      "source": [
        "ground_truth = []\n",
        "\n",
        "gt_df = valid_df.loc[valid_df['image_id'] == image_file]\n",
        "counter, boxes, labels, scores, keep = 0, [], [], [], [] \n",
        "for index, row in gt_df.iterrows():\n",
        "  boxes.append(np.array([row[\"x\"], row[\"y\"], row[\"x\"] + row[\"w\"], row[\"y\"] + row[\"h\"]]).astype(np.int32))\n",
        "  labels.append(row[\"class\"])\n",
        "  scores.append(1)\n",
        "  keep.append(counter)\n",
        "\n",
        "  counter += 1\n",
        "\n",
        "ground_truth.append([np.array(boxes).astype(np.int32), np.array(labels).astype(np.int32), \n",
        "                     np.array(scores).astype(np.int32), np.array(keep).astype(np.int32)])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCe_aoJnRH6K"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "preds = my_model(image_torch)\n",
        "for prediction in preds:\n",
        "  boxes, labels, scores, keep = [], [], [], [] \n",
        "  # Get predictions from device and convert them to numpy\n",
        "  boxes = prediction['boxes'].cpu().detach().numpy().astype(np.int32)\n",
        "  labels = prediction['labels'].cpu().detach().numpy().astype(np.int32)\n",
        "  scores = prediction['scores'].cpu().detach().numpy().astype(np.int32)\n",
        "\n",
        "  # Use NMS to get rid of overlapping predictions\n",
        "  keep = torchvision.ops.nms(prediction['boxes'], prediction['scores'], prediction_threshold).cpu().detach().numpy().astype(np.int32)\n",
        "\n",
        "  predictions.append([boxes, labels, scores, keep])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiNhsLjF71tQ"
      },
      "source": [
        "colour_dict = {1:(220, 0, 0), 2:(0, 0, 220), 3:(0, 220, 0), 4:(0, 220, 220)}\n",
        "\n",
        "print('Visualised results from your custom model')\n",
        "figure_storage = [ground_truth, predictions]\n",
        "\n",
        "# Initialize matlplotlib plot\n",
        "fig, axs = plt.subplots(nrows=len(preds), ncols=2, figsize=(20, 9))\n",
        "\n",
        "# Show the image data in a subplot\n",
        "for ax_i, ax in enumerate(axs):\n",
        "  temp_image = image.copy()\n",
        "  boxes, labels, scores, keep = figure_storage[ax_i % 2][ax_i//2]\n",
        "\n",
        "  # Plot all predicted bounding boxes onto a matplotlib plot with corresponding class names\n",
        "  for b, box in enumerate(boxes):\n",
        "    if b not in keep:\n",
        "      continue\n",
        "    \n",
        "    cv2.rectangle(temp_image,\n",
        "                  (box[0], box[1]),\n",
        "                  (box[2], box[3]),\n",
        "                  colour_dict[labels[b]], 1)\n",
        "\n",
        "    draw_text(temp_image,\n",
        "              class_dict[labels[b]],\n",
        "              font_scale=1,\n",
        "              pos=(box[0], box[1] - 10),\n",
        "              text_color_bg=colour_dict[labels[b]])\n",
        "  ax.set_axis_off()\n",
        "  ax.imshow(temp_image)\n",
        "\n",
        "fig.tight_layout()\n",
        "# Show the figure on the screen    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5gwcuHyD02s"
      },
      "source": [
        "WOW, its been a ride, thank you for your attention if you got all the way down here :) In this introductory notebook you have learned:\n",
        "\n",
        "*   How to use the baseline model to detect and classify crops in the drone images\n",
        "*   How to visualise the predictions\n",
        "*   How to train your own model\n",
        "\n",
        "As for now you can start exploring the other provided datasets, try to play with this object detection training pipeline and get to know the problem and possibly think of some better solutions for object detection.\n",
        "\n",
        "In the following days we plan to share details on all the problems that will be part of this Omdena challenge. Stay tuned for more info!\n",
        "\n",
        "Take care and wish you wonderful Sunday,\n",
        "\n",
        "Martin\n"
      ]
    }
  ]
}